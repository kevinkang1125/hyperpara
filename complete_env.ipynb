{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ijcai2022nmmo import CompetitionConfig, scripted, TeamBasedEnv, Team\n",
    "import nmmo\n",
    "import numpy as np\n",
    "import copy\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces \n",
    "from nmmo.io import action\n",
    "\n",
    "class MyTrainEnv(gym.Env):\n",
    "    def __init__(self,env_config:dict):\n",
    "        self.config = CompetitionConfig()\n",
    "        self.team_env = TeamBasedEnv(self.config)\n",
    "        self.env_config = env_config\n",
    "        #observation_space = [spaces.MultiDiscrete(2*np.ones((129,129),dtype=np.int32))]*8\n",
    "        self.observation_space = spaces.Tuple(( spaces.Box(low=-np.infty,high=np.infty,shape=(129,129,40),dtype=np.float32),\n",
    "                                               spaces.Box(low=-np.infty,high=np.infty,shape=(88,),dtype=np.float32) \n",
    "                                             ))\n",
    "        self.action_space = spaces.Box(low=-np.infty,high=np.infty,shape=(80,),dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.obs_by_team = self.team_env.reset()\n",
    "#         print(self.obs_by_team)\n",
    "        self.n_tick = 0 \n",
    "        ##### establish the teams:\n",
    "        self.teams = []\n",
    "        self.teams += [MyAwesomeTeam(\"MyTeam\",self.config)]\n",
    "        team_config = self.env_config[\"teams\"]\n",
    "        ### 随机生成不同的配比\n",
    "        \n",
    "        for key, team_num in team_config.items():\n",
    "            if team_num>0:\n",
    "                designed_team = getattr(scripted,key)\n",
    "                self.teams += [designed_team(key+f\"-{i}\",self.config) for i in range(team_num)] \n",
    "        ############################\n",
    "        \n",
    "        for team in self.teams:\n",
    "            team.reset()\n",
    "        \n",
    "        # myteam population id = 0\n",
    "        obs_to_myteam = self.obs_by_team[0]\n",
    "        \n",
    "        \n",
    "        new_obs = self.teams[0].update_map(obs_to_myteam)\n",
    "        return new_obs\n",
    "        \n",
    "    def step(self,action):\n",
    "        \n",
    "        ######## reward\n",
    "        def cal_rwd(init, seg1, seg2, seg3, prev, curr):\n",
    "            k = [4 / (seg1 - init), 6 / (seg2 - seg1), 11 / (seg3 - seg2)]\n",
    "            seg = [seg1, seg2, seg3]\n",
    "\n",
    "            rwd = 0\n",
    "            for i in range(3):\n",
    "                if prev < seg[i]:\n",
    "                    incre = min(curr - prev, seg[i] - prev)\n",
    "                    rwd += k[i] * incre\n",
    "                    prev += incre\n",
    "                    if prev == curr:\n",
    "                        return rwd\n",
    "            \n",
    "            return rwd\n",
    "        \n",
    "        def find_exploration(exploration_map):\n",
    "            if exploration_map.sum() == 0:\n",
    "                return 0\n",
    "            x,y = np.where(exploration_map==1)\n",
    "            return max( np.max(x)-np.min(x), np.max(y)-np.min(y)  )    \n",
    "        \n",
    "        actions = {}\n",
    "        ####### convert the output of the policy into the true action\n",
    "        new_my_action = self.teams[0].act(action)\n",
    "        \n",
    "        \n",
    "        actions[0]=new_my_action\n",
    "        ############\n",
    "        \n",
    "        ####### obtain the actions of other teams\n",
    "        for team_index in range(1,16):\n",
    "            if self.obs_by_team.get(team_index):\n",
    "                other_actions = self.teams[team_index].act(self.obs_by_team[team_index])\n",
    "            else:\n",
    "                other_actions ={}\n",
    "            actions[team_index] = other_actions\n",
    "        self.obs_by_team_all = self.team_env.step(actions)\n",
    "        self.obs_by_team = self.obs_by_team_all[0]\n",
    "        if self.obs_by_team.get(0) != None:\n",
    "            obs_to_myteam = self.obs_by_team[0]\n",
    "\n",
    "            new_my_obs = self.teams[0].update_map(obs_to_myteam)\n",
    "\n",
    "            self.n_tick += 1\n",
    "            if self.n_tick<1024:\n",
    "                done = False\n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "            info = {}\n",
    "\n",
    "            ### hunting and fishing 21\n",
    "            rwd_hunting = cal_rwd(10, 20, 35, 50, self.teams[0].last_global_information[\"hunting_level\"], self.teams[0].global_information[\"hunting_level\"])\n",
    "            rwd_fishing = cal_rwd(10, 20, 35, 50, self.teams[0].last_global_information[\"fishing_level\"], self.teams[0].global_information[\"fishing_level\"])\n",
    "            rwd_resources = (rwd_hunting + rwd_fishing) / 2\n",
    "\n",
    "            ### defeat players 21\n",
    "            rwd_kill = cal_rwd(0, 1, 3, 6, self.teams[0].last_global_information[\"kill_opponent_num\"], self.teams[0].global_information[\"kill_opponent_num\"])\n",
    "            print(\"kill_num_prev\",self.teams[0].last_global_information[\"kill_opponent_num\"])\n",
    "            print(\"kill_num_curr\",self.teams[0].global_information[\"kill_opponent_num\"])\n",
    "            ### exploration 21\n",
    "            for idx in range(8):\n",
    "                exploration_map = self.teams[0].global_map[\"agents\"][\"layers\"][:,:,idx*2+1]\n",
    "                self.teams[0].global_information[\"exploration\"] = max(find_exploration(exploration_map), self.teams[0].global_information[\"exploration\"])\n",
    "            rwd_exploration = cal_rwd(0, 32, 64, 127, self.teams[0].last_global_information[\"exploration\"], self.teams[0].global_information[\"exploration\"])\n",
    "#             self.teams[0].last_global_information[\"exploration\"] = self.teams[0].global_information[\"exploration\"]\n",
    "\n",
    "            ### food and water\n",
    "            rwd_food_and_water = 0\n",
    "            for idx in range(8):\n",
    "                food = self.teams[0].global_map['agents']['vectors'][idx*11+2]\n",
    "                water = self.teams[0].global_map['agents']['vectors'][idx*11+3]\n",
    "                if food < 0.3 * self.teams[0].global_information[\"hunting_level\"]:\n",
    "                    rwd_food_and_water += -0.02\n",
    "                if water < 0.3 * self.teams[0].global_information[\"fishing_level\"]:\n",
    "                    rwd_food_and_water += -0.02\n",
    "\n",
    "            ### equipment\n",
    "            rwd_equip = 0\n",
    "            if self.teams[0].global_information['kill_npc_level'] > self.teams[0].last_global_information['kill_npc_level']:\n",
    "                rwd_equip = cal_rwd(0, 1, 10, 20, self.teams[0].last_global_information[\"kill_npc_level\"], self.teams[0].global_information[\"kill_npc_level\"])\n",
    "\n",
    "            ### level\n",
    "            alive = False\n",
    "            rwd_level = 0\n",
    "            for idx in range(8):\n",
    "                level = self.teams[0].global_map['agents']['vectors'][idx*11+0]\n",
    "                if level > 0:\n",
    "                    alive = True\n",
    "                self.teams[0].global_information['level'] = max(self.teams[0].global_information['level'], level)\n",
    "            rwd_level += (self.teams[0].global_information['level'] - self.teams[0].last_global_information['level']) * 21 / 80\n",
    "            self.teams[0].last_global_information['level'] = self.teams[0].global_information['level']\n",
    "\n",
    "            ### alive\n",
    "            rwd_alive = 0\n",
    "            if alive:\n",
    "                rwd_alive += 0.02\n",
    "\n",
    "            reward = rwd_resources + rwd_kill + rwd_exploration + rwd_food_and_water + rwd_equip + rwd_level + rwd_alive\n",
    "            print(\"---------------------------------\")\n",
    "            print( \"resources: \",rwd_resources)\n",
    "            print( \"kill: \",rwd_kill)\n",
    "            print( \"exploration: \",rwd_exploration)\n",
    "            print( \"food_and_water: \",rwd_food_and_water)\n",
    "            print( \"equip: \",rwd_equip)\n",
    "            print( \"level: \",rwd_level)\n",
    "            print( \"alive: \",rwd_alive)\n",
    "            print(\"---------------------------------\")\n",
    "            \n",
    "        else:\n",
    "            done = True   \n",
    "            reward = 0 \n",
    "            info = {}\n",
    "            new_my_obs = (np.zeros((129,129,40)),np.zeros(88))\n",
    "#         print(done)\n",
    "        return new_my_obs, reward, done, info\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be59b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################\n",
    "## Myteam\n",
    "class MyAwesomeTeam(Team):\n",
    "    def reset(self):\n",
    "        # reset some states\n",
    "        self.global_map = {}\n",
    "        self.n_tick = 0 \n",
    "        self.LOCAL_BIAS = 16\n",
    "        self.global_information={ \n",
    "                                  \"level\": 1,\n",
    "                                  \"hunting_level\":10, \n",
    "                                  \"fishing_level\":10,\n",
    "                                  \"kill_npc_num\":0,\n",
    "                                  \"kill_npc_level\":0,\n",
    "                                  \"kill_opponent_num\":0,\n",
    "                                  \"exploration\":0\n",
    "                                  }\n",
    "        \n",
    "        self.last_global_information={ \"hunting_level\":10, \n",
    "                               \"fishing_level\":10,\n",
    "                               \"kill_npc_level\":0,\n",
    "                               \"kill_npc_num\":0,\n",
    "                               \"kill_opponent_num\":0,\n",
    "                               \"exploration\":0,\n",
    "                               \"level\": 1}\n",
    "        \n",
    "        self.agents_eval = np.zeros((8,6),dtype=np.float32)\n",
    "        self.agents_eval[:,0] = 1 # level\n",
    "        self.agents_eval[:,1] = 10 # hunting level\n",
    "        self.agents_eval[:,2] = 10 # fishing level\n",
    "        self.agents_eval[:,3] = 0 # kill_npc_num\n",
    "        self.agents_eval[:,4] = 0 # kill_npc_level\n",
    "        self.agents_eval[:,5] = 0 # kill_opponents_num \n",
    "        \n",
    "        self.last_action_informations= np.zeros((8,2),dtype=np.int32)  # id-level\n",
    "        self.agents_pos = np.zeros((8, 2))  # agent position\n",
    "    \n",
    "    def obs_normalization(self,agent_layers,agent_vectors,opponents,npcs,terrain):\n",
    "        MAX_LEVEL = 80\n",
    "        MAX_HEALTH = MAX_LEVEL\n",
    "        MAX_FOOD = 80 \n",
    "        MAX_WATER = 80\n",
    "        MAX_DAMAGE = 1000\n",
    "        MAX_HUNTING_LEVEL = 75 \n",
    "        MAX_FISHING_LEVEL = 75 \n",
    "        MAX_KILL_NUM = 20\n",
    "        \n",
    "        # normalization for the agents\n",
    "        for agent_index in range(8):\n",
    "            # level\n",
    "            agent_vectors[agent_index*11+0] /= MAX_LEVEL\n",
    "            # health\n",
    "            agent_vectors[agent_index*11+1] /= MAX_HEALTH\n",
    "            # food\n",
    "            agent_vectors[agent_index*11+2] /= MAX_FOOD\n",
    "            # water \n",
    "            agent_vectors[agent_index*11+3] /= MAX_WATER\n",
    "            # DAMAGE \n",
    "            agent_vectors[agent_index*11+4] /= MAX_DAMAGE\n",
    "            # HUNTING_LEVEL\n",
    "            agent_vectors[agent_index*11+6] /= MAX_HUNTING_LEVEL\n",
    "            # FISHING LEVEL \n",
    "            agent_vectors[agent_index*11+7] /= MAX_FISHING_LEVEL\n",
    "            # KILL_NPC_LEVEL\n",
    "            agent_vectors[agent_index*11+8] /= MAX_LEVEL\n",
    "            # kill_npc_num\n",
    "            agent_vectors[agent_index*11+9] /= MAX_KILL_NUM\n",
    "            # KILL_OPPONENT_NUM\n",
    "            agent_vectors[agent_index*11+10] /= MAX_KILL_NUM\n",
    "        \n",
    "        MAX_DIS_NUM = 5\n",
    "        # normalization for the opponents AND NPCS \n",
    "        # distribution \n",
    "        opponents[:,:,0] /= MAX_DIS_NUM \n",
    "        npcs[:,:,0] /= MAX_DIS_NUM\n",
    "        # level \n",
    "        opponents[:,:,1] /= MAX_LEVEL\n",
    "        npcs[:,:,1] /= MAX_LEVEL\n",
    "        # damage\n",
    "        opponents[:,:,2] /= MAX_DAMAGE\n",
    "        npcs[:,:,2] /= MAX_DAMAGE\n",
    "        # food \n",
    "        opponents[:,:,3] /= MAX_FOOD\n",
    "        npcs[:,:,3] /= MAX_FOOD\n",
    "        # water \n",
    "        opponents[:,:,4] /= MAX_WATER\n",
    "        npcs[:,:,4] /= MAX_WATER\n",
    "        # health\n",
    "        opponents[:,:,5] /= MAX_HEALTH\n",
    "        npcs[:,:,5] /= MAX_HEALTH\n",
    "        \n",
    "        ##### clip between [0,1]\n",
    "        agent_layers = np.clip(agent_layers,0,1)\n",
    "        opponents = np.clip(opponents,0,1)\n",
    "        npcs = np.clip(npcs,0,1)\n",
    "        terrain = np.clip(terrain,0,1)\n",
    "        agent_vectors = np.clip(agent_vectors,0,1)\n",
    "        \n",
    "        # concatenate\n",
    "        obs_layers = np.concatenate( (agent_layers,opponents,npcs,terrain),axis=-1 )\n",
    "        \n",
    "        return ( obs_layers, agent_vectors )\n",
    "            \n",
    "    \n",
    "    def update_map(self,observations:dict[int,dict]):\n",
    "        ############# agent layer information\n",
    "        agent_visible = np.zeros([129,129])\n",
    "        agent_info = np.zeros([129,129,16])\n",
    "        agents_dis = np.zeros([129,129])\n",
    "        ############# agent vector information\n",
    "        agent_vectors = np.zeros([88])\n",
    "        ############# opponent layer information\n",
    "        opponents=np.zeros([129,129,7])\n",
    "        ############# npc layer infromation\n",
    "        npcs=np.zeros([129,129,7])\n",
    "        ############# record the attack objects \n",
    "        self.attack_objs = []\n",
    "        for k in self.global_information:\n",
    "            self.last_global_information[k] = self.global_information[k]\n",
    "        for agent_index in range(8):\n",
    "            attack_obj = []\n",
    "            last_object_list = []\n",
    "            if observations.get(agent_index)!=None:\n",
    "                ######## obttain the obs from the env\n",
    "#                 print(observations[agent_index])\n",
    "                obs_en = observations[agent_index]['Entity']['Continuous']\n",
    "                ######## team pop \n",
    "                team_id = int( obs_en[0][4] )\n",
    "                ########  agent row | column index \n",
    "                agent_row_index = int( obs_en[0][5]- self.LOCAL_BIAS )\n",
    "                agent_column_index = int( obs_en[0][6] - self.LOCAL_BIAS )\n",
    "                self.agents_pos[agent_index] = np.array([agent_row_index, agent_column_index])\n",
    "                ######## opponents, npcs location\n",
    "                opponents_loc = []\n",
    "                npcs_loc = []\n",
    "                \n",
    "                ######## information for npcs killing and opponents killing\n",
    "                \n",
    "                ######## \n",
    "                for index in range(100):\n",
    "                    info = int(obs_en[index][0])\n",
    "                    if info==0:\n",
    "                        break\n",
    "                    entity_id = int(obs_en[index][1]) # entity_id\n",
    "                    level = int( obs_en[index][3] )   # agent level \n",
    "                    pop = int( obs_en[index][4])      # agent pop \n",
    "                    row_index = int( obs_en[index][5]- self.LOCAL_BIAS )  # row index \n",
    "                    column_index = int( obs_en[index][6] - self.LOCAL_BIAS ) # column index \n",
    "                    damage = int( obs_en[index][7] )  # damage received \n",
    "                    timealive = int( obs_en[index][8] ) # timealive\n",
    "                    food = int( obs_en[index][9] )  # food \n",
    "                    water = int( obs_en[index][10] ) # water\n",
    "                    health = int( obs_en[index][11] ) # health \n",
    "                    frozen = int( obs_en[index][12] ) # frozen \n",
    "\n",
    "                    if index==0:\n",
    "                        # 表示是该team的agent\n",
    "                        ### update layers \n",
    "                        ######### 更新 agent_visible\n",
    "                        agent_visible[int(max(row_index-7,0)):int(min(row_index+8,129)),\n",
    "                                      int(max(column_index-7,0)):int(min(column_index+8,129))] = 1\n",
    "                        ######### update agent info\n",
    "                        ################################   alive\n",
    "                        agent_info[row_index][column_index][agent_index*2+0]=1\n",
    "                        ################################   exploration\n",
    "                        if self.global_map=={}: # first step\n",
    "                            agent_info[:,:,agent_index*2+1]=agent_visible\n",
    "                        else:\n",
    "                            agent_info[:,:,agent_index*2+1] = self.global_map[\"agents\"][\"layers\"][:,:,agent_index*2+1]\n",
    "                            agent_info[int(max(row_index-7,0)):int(min(row_index+8,129)),\n",
    "                                      int(max(column_index-7,0)):int(min(column_index+8,129)),\n",
    "                                      agent_index*2+1] = 1\n",
    "                            \n",
    "                        ######### agent 更新 agent dis\n",
    "                        agents_dis[row_index][column_index]=1 # locations \n",
    "                        \n",
    "                        ### update agent vectors \n",
    "                        ######### agent 更新 agent level\n",
    "                        agent_vectors[agent_index*11+0]=level\n",
    "                        self.agents_eval[agent_index,0] = max(self.agents_eval[agent_index,0],level)\n",
    "                        ######### agent 更新 agent health\n",
    "                        agent_vectors[agent_index*11+1]=health\n",
    "                        ######### agent 更新 agent food\n",
    "                        agent_vectors[agent_index*11+2]=food                        \n",
    "                        ######### agent 更新 agent water\n",
    "                        agent_vectors[agent_index*11+3]=water\n",
    "                        ######### agent 更新 agent damage\n",
    "                        agent_vectors[agent_index*11+4]=damage                        \n",
    "                        ######### agent 更新 agent frozen\n",
    "                        agent_vectors[agent_index*11+5]=frozen  \n",
    "                        \n",
    "                        ######### agent 更新 hunting level\n",
    "                        self.agents_eval[agent_index,1] = max(food,self.agents_eval[agent_index,1])\n",
    "                        agent_vectors[agent_index*11+6] = self.agents_eval[agent_index,1]\n",
    "                        \n",
    "                        ######### agent 更新 fishing level\n",
    "                        self.agents_eval[agent_index,2] = max(water,self.agents_eval[agent_index,2])\n",
    "                        agent_vectors[agent_index*11+7] = self.agents_eval[agent_index,2]\n",
    "\n",
    "                        ######### agent 更新 timealive\n",
    "                        self.n_tick = timealive\n",
    "\n",
    "                        #############################################################\n",
    "                    else:\n",
    "                        if pop>0 and pop!=team_id: # opponent\n",
    "                            ######## opponent distribution\n",
    "                            opponents[row_index][column_index][0]+=1\n",
    "                            ######## level\n",
    "                            opponents[row_index][column_index][1]+=level\n",
    "                            ######## damage\n",
    "                            opponents[row_index][column_index][2]+=damage\n",
    "                            ######## food\n",
    "                            opponents[row_index][column_index][3]+=food\n",
    "                            ######## water\n",
    "                            opponents[row_index][column_index][4]+=water                                \n",
    "                             ######## health\n",
    "                            opponents[row_index][column_index][5]+=health\n",
    "                            ######## frozen\n",
    "                            opponents[row_index][column_index][6]+=frozen\n",
    "\n",
    "                            ######## killing opponents\n",
    "                            if self.last_action_informations[agent_index][0]>0:\n",
    "                                last_object_list.append(entity_id)\n",
    "                            \n",
    "                            if [row_index,column_index] not in opponents_loc:\n",
    "                                opponents_loc.append( [row_index,column_index] )\n",
    "                            if max(abs(row_index-agent_row_index),abs(column_index-agent_column_index))<=4:\n",
    "                                attack_obj.append([entity_id,\n",
    "                                                   abs(row_index-agent_row_index),\n",
    "                                                   abs(column_index-agent_column_index),\n",
    "                                                  level,\n",
    "                                                  damage,\n",
    "                                                  food,\n",
    "                                                  water,\n",
    "                                                  health,\n",
    "                                                  frozen,\n",
    "                                                   index])\n",
    "                        elif pop<0: # npcs \n",
    "                            ######## npc distribution\n",
    "                            npcs[row_index][column_index][0]+=1\n",
    "                            ######## level\n",
    "                            npcs[row_index][column_index][1]+=level\n",
    "                            ######## damage\n",
    "                            npcs[row_index][column_index][2]+=damage\n",
    "                            ######## food\n",
    "                            npcs[row_index][column_index][3]+=food\n",
    "                            ######## water\n",
    "                            npcs[row_index][column_index][4]+=water                                \n",
    "                            ######## health\n",
    "                            npcs[row_index][column_index][5]+=health\n",
    "                            ######## frozen\n",
    "                            npcs[row_index][column_index][6]+=frozen      \n",
    "                            \n",
    "                            ############ killing npcs \n",
    "                            if self.last_action_informations[agent_index][0]<0:\n",
    "                                last_object_list.append(entity_id)\n",
    "                                \n",
    "                            if [row_index,column_index] not in npcs_loc:\n",
    "                                npcs_loc.append([row_index,column_index])\n",
    "                                \n",
    "                            if max(abs(row_index-agent_row_index), abs(column_index-agent_column_index))<=4:\n",
    "                                attack_obj.append([entity_id,\n",
    "                                                   abs(row_index-agent_row_index),\n",
    "                                                   abs(column_index-agent_column_index),\n",
    "                                                  level,\n",
    "                                                  damage,\n",
    "                                                  food,\n",
    "                                                  water,\n",
    "                                                  health,\n",
    "                                                  frozen,\n",
    "                                                   index])\n",
    "                \n",
    "                for loc in opponents_loc:\n",
    "                    for i in range(1,7):\n",
    "                        opponents[loc[0]][loc[1]][i] = int( opponents[loc[0]][loc[1]][i]/opponents[loc[0]][loc[1]][0])\n",
    "                for loc in npcs_loc:\n",
    "                    for i in range(1,7):\n",
    "                        npcs[loc[0]][loc[1]][i] = int(npcs[loc[0]][loc[1]][i]/npcs[loc[0]][loc[1]][0])\n",
    "            ### update the killing information:\n",
    "            if last_object_list != []:\n",
    "                if self.last_action_informations[agent_index][0]>0 and \\\n",
    "                   self.last_action_informations[agent_index][0] not in last_object_list :\n",
    "                    self.agents_eval[agent_index,5]+=1 \n",
    "                elif self.last_action_informations[agent_index][0]<0 and \\\n",
    "                   self.last_action_informations[agent_index][0] not in last_object_list :\n",
    "                    self.agents_eval[agent_index,3]+=1\n",
    "                    self.agents_eval[agent_index,4] = max(self.agents_eval[agent_index,4],\n",
    "                                                                   self.last_action_informations[agent_index][1])\n",
    "            \n",
    "            ####### update the killing information:\n",
    "            ### kill_npc level \n",
    "            agent_vectors[agent_index*11+8] = self.agents_eval[agent_index,4]\n",
    "            ### kill_npc_num\n",
    "            agent_vectors[agent_index*11+9] = self.agents_eval[agent_index,3]\n",
    "            ### kill_opponent_num \n",
    "            agent_vectors[agent_index*11+10] = self.agents_eval[agent_index,5]\n",
    "            \n",
    "            self.attack_objs.append(np.array(attack_obj,dtype=np.float32))\n",
    "        ##################### terrain update\n",
    "        if self.global_map=={}: # first step\n",
    "            terrain = np.zeros([129,129,8])  \n",
    "            for agent_index in range(8):\n",
    "                if observations.get(agent_index)!=None:\n",
    "                    obs_til = observations[agent_index]['Tile']['Continuous']\n",
    "                    for index in range(225):\n",
    "                        til_type = int( obs_til[index][1] )\n",
    "                        til_row = int( obs_til[index][2] -self.LOCAL_BIAS )\n",
    "                        til_column = int( obs_til[index][3] -self.LOCAL_BIAS ) \n",
    "                        if til_row<0 or til_row > 128 or til_column<0 or til_column>128:\n",
    "                            continue \n",
    "\n",
    "                        terrain[til_row][til_column][0]= 1\n",
    "                        terrain[til_row][til_column][til_type]=1\n",
    "        else:\n",
    "            terrain = self.global_map['terrain']\n",
    "            for i in range(129):\n",
    "                for j in range(129):\n",
    "                    if terrain[i][j][3]==1:\n",
    "                        terrain[i][j][6]=1\n",
    "                    if terrain[i][j][4]==1:\n",
    "                        terrain[i][j][7]=1       \n",
    "            for agent_index in range(8):     \n",
    "                if observations.get(agent_index)!=None:\n",
    "                    obs_til = observations[agent_index]['Tile']['Continuous']\n",
    "                    for index in range(225):\n",
    "                        til_type = int( obs_til[index][1] )\n",
    "                        til_row = int( obs_til[index][2] -self.LOCAL_BIAS )\n",
    "                        til_column = int( obs_til[index][3] -self.LOCAL_BIAS ) \n",
    "                        if til_row<0 or til_row > 128 or til_column<0 or til_column>128:\n",
    "                            continue \n",
    "\n",
    "                        terrain[til_row][til_column][0]= 1\n",
    "                        terrain[til_row][til_column][til_type]=1\n",
    "\n",
    "                        if til_type==3 or til_type==4:\n",
    "                            terrain[til_row][til_column][6]=0\n",
    "                            terrain[til_row][til_column][6]=0\n",
    "        ######## update the global information\n",
    "        self.global_information={ \n",
    "                                  \"level\": 1,\n",
    "                                  \"hunting_level\":10, \n",
    "                                  \"fishing_level\":10,\n",
    "                                  \"kill_npc_num\":0,\n",
    "                                  \"kill_npc_level\":0,\n",
    "                                  \"kill_opponent_num\":0,\n",
    "                                  \"exploration\":0\n",
    "                                  }        \n",
    "        \n",
    "        self.global_information[\"level\"]=np.max(self.agents_eval[:,0])\n",
    "        self.global_information[\"hunting_level\"]=np.max(self.agents_eval[:,1])\n",
    "        self.global_information[\"fishing_level\"]=np.max(self.agents_eval[:,2])\n",
    "        self.global_information[\"kill_npc_num\"]=np.max(self.agents_eval[:,3])\n",
    "        self.global_information[\"kill_npc_level\"]=np.max(self.agents_eval[:,4])\n",
    "        self.global_information[\"kill_opponent_num\"]=np.max(self.agents_eval[:,5])\n",
    "        \n",
    "        ######## concatenate                        \n",
    "        self.global_map['terrain']=terrain\n",
    "        self.global_map['oppos']=opponents\n",
    "        self.global_map['npcs']=npcs\n",
    "        \n",
    "        agent_visible_expanded = np.expand_dims(agent_visible,axis=-1)\n",
    "        agents_dis_expanded = np.expand_dims(agents_dis,axis=-1)\n",
    "        agent_layers = np.concatenate( (agent_info,agent_visible_expanded,agents_dis_expanded),axis=-1 )\n",
    "        self.global_map['agents']={\"layers\":agent_layers, \"vectors\":agent_vectors}\n",
    "\n",
    "        obs = self.obs_normalization(copy.deepcopy(agent_layers),\n",
    "                                     copy.deepcopy(agent_vectors),\n",
    "                                     copy.deepcopy(opponents),\n",
    "                                     copy.deepcopy(npcs),\n",
    "                                     copy.deepcopy(terrain))\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def show_map(self,characters,agent_attris,attris,agent_index=0,terrain_option=False,ter_attris=[0,1,2,3,4,5,6,7]):\n",
    "        \"\"\"\n",
    "        characters :  [\"agents\",\"oppos\",\"npcs\"]\n",
    "        attris = [0,1,2,3,4,5,6]\n",
    "        ter_attris = [0,1,2,3,4,5,6,7]\n",
    "        \"\"\"\n",
    "        \n",
    "        CHARAC = ['distribution','level','damage','food','water','health','is_frozen']\n",
    "        AGENTS = [\"location\",\"exploration\",'visible','agents distribution']\n",
    "        TERRAIN = [\"exploration\",\"water\",\"grass\",\"scrub(latest)\",\"forest(latest)\",\n",
    "                   \"stone\",\"scrub(previous)\",\"forest(previous)\"]\n",
    "        i = 0\n",
    "        for character in characters:\n",
    "            if character == \"agents\":\n",
    "                for attri in agent_attris:\n",
    "                    plt.figure(i)\n",
    "                    i+=1\n",
    "                    ######## show agents \n",
    "                    plt.imshow(self.global_map[character][\"layers\"][:,:,agent_index*2+attri],cmap=plt.cm.cool)\n",
    "                    plt.colorbar()\n",
    "                    plt.title(character+\" \"+AGENTS[attri])\n",
    "                    plt.show()\n",
    "                    \n",
    "                plt.figure(i)\n",
    "                i+=1\n",
    "                plt.imshow(self.global_map[character][\"layers\"][:,:,-2],cmap=plt.cm.cool)\n",
    "                plt.colorbar()\n",
    "                plt.title(AGENTS[-2])\n",
    "                \n",
    "                plt.figure(i)\n",
    "                i+=1\n",
    "                plt.imshow(self.global_map[character][\"layers\"][:,:,-1],cmap=plt.cm.cool)\n",
    "                plt.colorbar()\n",
    "                plt.title(AGENTS[-1])                \n",
    "            else:\n",
    "                for attri in attris:\n",
    "                    plt.figure(i)\n",
    "                    i+=1\n",
    "                    plt.imshow(self.global_map[character][:,:,agent_index*8+attri],cmap=plt.cm.cool)\n",
    "                    plt.colorbar()\n",
    "                    plt.title(character+\" \"+CHARAC[attri])\n",
    "                    plt.show()    \n",
    "        if terrain_option:\n",
    "            terrains = self.global_map['terrain']\n",
    "            for ter_attri in ter_attris:\n",
    "                plt.figure(i)\n",
    "                i+=1     \n",
    "                \n",
    "                plt.imshow(terrains[:,:,ter_attri],cmap=plt.cm.cool)\n",
    "                plt.colorbar()\n",
    "                plt.title(\"terrain \"+TERRAIN[ter_attri])\n",
    "                    \n",
    "                    \n",
    "    def act(self, actions: dict[int, dict]) -> dict[int, dict]:\n",
    "        # modified: actions: 8 * 10 [x, y, x_w, y_w, LEVEL_w, damage_w, food_w, WATER_W, health_w, frozen_w]\n",
    "        # self.attack_objs[i]: n_bojs * 9 [id, x, y, level, damage, food, water, health, frozen, idx] \n",
    "        \n",
    "        actions = actions.reshape((8, 10))\n",
    "        actions_output = {}\n",
    "        for i, a in enumerate(actions):\n",
    "            # 初始化输出的action\n",
    "            action_output = {action.Attack:{\n",
    "                                 action.Style: None,\n",
    "                                 action.Target: None\n",
    "                                 },\n",
    "                             action.Move: {\n",
    "                                 action.Direction: None\n",
    "                                 }\n",
    "                             }\n",
    "            \n",
    "            # 每个agent的四个移动方向 [North, South, East, West]\n",
    "            move = 0\n",
    "            if np.abs(a[0]) > np.abs(a[1]) and a[0] > 0:\n",
    "                move = 2 # east\n",
    "            elif np.abs(a[0]) > np.abs(a[1]) and a[0] < 0:\n",
    "                move = 3 # west\n",
    "            elif np.abs(a[0]) < np.abs(a[1]) and a[1] > 0:\n",
    "                move = 0 # north\n",
    "            elif np.abs(a[0]) < np.abs(a[1]) and a[1] < 0:\n",
    "                move = 1 # south\n",
    "            \n",
    "            action_output[action.Move][action.Direction] = move\n",
    "            central_dist = np.linalg.norm(a[:2])\n",
    "            if central_dist <= 0.1 or (self.agents_pos[i][0] == 0 and move == 0) \\\n",
    "                                   or (self.agents_pos[i][0] == 128 and move == 1) \\\n",
    "                                   or (self.agents_pos[i][1] == 0 and move == 3) \\\n",
    "                                   or (self.agents_pos[i][1] == 128 and move == 2):\n",
    "                action_output.pop(action.Move)        \n",
    "            \n",
    "            # 每个agent的攻击对象\n",
    "            if self.attack_objs[i].size != 0:\n",
    "                score = np.sum(a[2:] * self.attack_objs[i][:,1:9], 1)  # score\n",
    "                dist = np.max(self.attack_objs[i][:,1:3], 1)\n",
    "                # sort\n",
    "                max_idx = np.argsort(score)[::-1][0]\n",
    "                dist = dist[max_idx]\n",
    "                if dist <= 1:\n",
    "                    action_output[action.Attack][action.Style] = 0\n",
    "                elif dist <= 3:\n",
    "                    action_output[action.Attack][action.Style] = 1\n",
    "                elif dist <= 4:\n",
    "                    action_output[action.Attack][action.Style] = 2\n",
    "                action_output[action.Attack][action.Target] = int(self.attack_objs[i][max_idx][9])\n",
    "                self.last_action_informations[i,0] = int(self.attack_objs[i][max_idx][0])\n",
    "                self.last_action_informations[i,1] = int(self.attack_objs[i][max_idx][3])\n",
    "            else:\n",
    "                action_output.pop(action.Attack)\n",
    "                self.last_action_informations[i,0] = 0\n",
    "                self.last_action_informations[i,1] = 0\n",
    "                \n",
    "                \n",
    "            actions_output[i] = action_output\n",
    "        \n",
    "        return actions_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96173202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e79946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "import tqdm\n",
    "import json5\n",
    "import ray.tune as tune \n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "import numpy as np\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "##### register env    \n",
    "def trainenv_creator(env_config):\n",
    "    return MyTrainEnv(env_config)\n",
    "\n",
    "# ray.shutdown()\n",
    "# ray.init(num_cpus=3,ignore_reinit_error=True,object_store_memory=3*1024*1024*1024)\n",
    "\n",
    "algo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "######### load config \n",
    "with open(\"config.json\") as json_file:\n",
    "    all_config = json5.load(json_file)\n",
    "\n",
    "for key,value in all_config[\"algo_config\"].items():\n",
    "    if isinstance(value,dict):\n",
    "        for sub_key,sub_value in value.items():\n",
    "            algo_config[key][sub_key] = sub_value\n",
    "    else:\n",
    "        algo_config[key] = value\n",
    "\n",
    "algo_config[\"env_config\"] = all_config[\"env_config\"]\n",
    "algo_config[\"model\"][\"fcnet_activation\"] = None\n",
    "# env = MyTrainEnv(algo_config[\"env_config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7734c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc20755",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.teams[0].show_map([\"agents\",\"oppos\",\"npcs\"],[0,1],[0,1,2,3,4,5,6],agent_index=0,terrain_option=True,ter_attris=[0,1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1fdb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step( env.action_space.sample() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.obs_by_team_all[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.teams[0].show_map([\"agents\",\"oppos\",\"npcs\"],[0,1],[0,1,2,3,4,5,6],agent_index=0,terrain_option=True,ter_attris=[0,1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53beab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "register_env(\"MyTrainEnv\",trainenv_creator)\n",
    "# trainer = ppo.PPOTrainer(env=\"MyTrainEnv\",config=all_config[\"algo_config\"] )# config to pass to env class\n",
    "\n",
    "# algo_config[\"env\"] = \"MyTrainEnv\"\n",
    "# algo_config[\"lr\"] = tune.grid_search([0.001,0.0005])\n",
    "algo_config[\"lr\"] = 0.001\n",
    "algo_config[\"ignore_worker_failures\"]=True\n",
    "algo_config[\"recreate_failed_workers\"]=True\n",
    "algo_config[\"preprocessor_pref\"]=None\n",
    "algo_config[\"train_batch_size\"] = 128\n",
    "\n",
    "trainer = ppo.PPOTrainer(env=\"MyTrainEnv\",config={\"framework\":\"torch\",\n",
    "                                                  \"train_batch_size\":128,\n",
    "                                                  \"lr\":0.001,\n",
    "                                           \"env_config\":all_config[\"env_config\"],\n",
    "                                                   \"model\":{\n",
    "                                                            \"fcnet_hiddens\":[],\n",
    "                                                       \"fcnet_activation\":None,\n",
    "                                                       \n",
    "                                                       \"conv_filters\":[[16,[129,129],1] ],\n",
    "                                                       \"conv_activation\":\"relu\",\n",
    "                                                       \n",
    "                                                       \"post_fcnet_hiddens\":[ 100,100 ],\n",
    "                                                       \"post_fcnet_activation\":\"relu\"\n",
    "                                                    }\n",
    "                                                  } \n",
    "                        )# config to pass to env class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ppo.PPOTrainer(env=\"MyTrainEnv\",config=algo_config)\n",
    "\n",
    "# tune.run('PPO',\n",
    "#          config=algo_config,\n",
    "#          stop={\"timesteps_total\":4000}\n",
    "#         )\n",
    "for i in range(200):\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n",
    "    if i%20 == 0: \n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4eafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.get_policy().model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array( [[2,2,3,7,5],[6,3,6,7,2],[3,2,7,6,9]] )\n",
    "x,y = np.where(a==7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37010afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19746dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijcai2022-nmmo",
   "language": "python",
   "name": "ijcai2022-nmmo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
